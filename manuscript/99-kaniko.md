## TODO

- [ ] Code
- [ ] Write
- [ ] Code review GKE
- [ ] Code review EKS
- [ ] Code review AKS
- [ ] Code review existing cluster
- [ ] Text review
- [ ] Gist
- [ ] Review titles
- [ ] Proofread
- [ ] Diagrams
- [ ] Add to slides
- [ ] Publish on TechnologyConversations.com
- [ ] Add to Book.txt
- [ ] Publish on LeanPub.com

# kaniko

kaniko is a tool to build container images from a `Dockerfile`, similar to `docker build`, but without needing a Docker daemon. kaniko builds the images inside a container, executing the `Dockerfile` commands in userspace, so it allows us to build the images in standard Kubernetes clusters.

This means that in a containerized environment, be it a Kubernetes cluster, a Jenkins agent running in Docker, or any other container scheduler, we no longer need to use Docker in Docker nor do the build in the host system by mounting the Docker socket, simplifying and improving the security of container image builds.

Still, kaniko does not make it safe to run untrusted container image builds, but it relies on the security features of the container runtime.
If you have a minimal base image that doesn't require permissions to unpack, and your Dockerfile doesn't execute any commands as the root user, you can run Kaniko without root permissions.

kaniko builds the container image inside a container, so it needs a way to get the build context (the directory where the Dockerfile and any other files that we want to copy into the container are) and to push the resulting image to a registry.

The build context can be a compressed tar in a Google Cloud Storage or AWS S3 bucket, a local directory inside the kaniko container, that we need to mount ourselves, or a git repository.

kaniko can be run in Docker, Kubernetes, Google Cloud Build (sending our image build to Google Cloud), or gVisor.
gVisor is an [OCI](https://www.opencontainers.org/) sandbox runtime that provides a virtualized container environment. It provides an additional security boundary for our container image builds.

Images can be pushed to any standard Docker registry but also Google GCR and AWS ECR are directly supported.

With Docker daemon image builds (`docker build`) we have caching. Each layer generated by `RUN` commands in the Dockerfile is kept and reused if the commands don't change. In kaniko, because the image builds happen inside a container that is gone after the build we lose anything built locally. To solve this, kaniko can push these intermediate layers resulting from `RUN` commands to the remote registry when using the `--cache` flag.


## Cluster

* Create new **GKE** cluster: [gke-jx-kaniko.sh](TODO:)
* Create new **EKS** cluster: [eks-jx-kaniko.sh](TODO:)
* Create new **AKS** cluster: [aks-jx-kaniko.sh](TODO:)
* Use an **existing** cluster: [install-kaniko.sh](TODO:)
* Use an **existing** cluster: [upgrade-kaniko.sh](TODO:)

```bash
# cd go-demo-6

# git pull

# git checkout pr

# git merge -s ours master --no-edit

# git checkout master

# git merge pr

# git push
```

```bash
# jx import --batch-mode

# jx get activities --filter go-demo-6 --watch
```

## Kaniko in Docker

We can run kaniko from a machine with Docker daemon installed for testing or validation.

```bash
cd go-demo-6 # If you're not already there
make

# if you just want to test the build, no pushing
docker run \
    -v `pwd`:/workspace gcr.io/kaniko-project/executor:latest \
    --no-push

# if you want to push to GCR
PROJECT=[...] # Your Google Cloud project id
gcloud auth application-default login # get the Google Cloud credentials
docker run \
    -v $HOME/.config/gcloud:/root/.config/gcloud \
    -v `pwd`:/workspace \
    gcr.io/kaniko-project/executor:latest \
    --destination gcr.io/$PROJECT/go-demo-6:kaniko-docker \
    --cache

# if you want to push to DockerHub (similarly to other Docker registries)
DOCKER_USERNAME=[...]
DOCKER_PASSWORD=[...]
AUTH=$(echo -n "${DOCKER_USERNAME}:${DOCKER_PASSWORD}" | base64)
cat << EOF > config.json
{
    "auths": {
        "https://index.docker.io/v1/": {
            "auth": "${AUTH}"
        }
    }
}
EOF
docker run \
    -v `pwd`/config.json:/root/.docker/config.json \
    -v `pwd`:/workspace \
    gcr.io/kaniko-project/executor:latest \
    --destination $DOCKER_USERNAME/go-demo-6:kaniko-docker \
    --cache
```

## Kaniko in Kubernetes

In Kubernetes we can manually create a pod that will do our Docker image build.

Depending on where we want to push to we need to create the corresponding secrets.

For Dockerhub or a Docker registry

```bash
DOCKER_USERNAME=[...]
DOCKER_PASSWORD=[...]
DOCKER_SERVER=https://index.docker.io/v1/
kubectl create secret docker-registry regcred \
    --docker-server=${DOCKER_SERVER} \
    --docker-username=${DOCKER_USERNAME} \
    --docker-password=${DOCKER_PASSWORD}

# TODO vfarcic create the kaniko branch in vfarcic/go-demo-6

echo << EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: kaniko
spec:
  restartPolicy: Never
  containers:
  - name: kaniko
    image: gcr.io/kaniko-project/executor
    imagePullPolicy: Always
    args: ["--dockerfile=Dockerfile",
            "--context=git://github.com/carlossg/go-demo-6.git#refs/heads/kaniko",
            "--destination=csanchez/test"]
    volumeMounts:
      - name: docker-config
        mountPath: /kaniko/.docker
  volumes:
  - name: docker-config
    projected:
      sources:
      - secret:
          name: regcred
          items:
            - key: .dockerconfigjson
              path: config.json
EOF
```

TODO Kaniko on GCR with caching
TODO Kaniko on AWS


## What Now?

```bash
cd ..

GH_USER=[...]

hub delete -y \
  $GH_USER/environment-jx-rocks-staging

hub delete -y \
  $GH_USER/environment-jx-rocks-production
  
hub delete -y $GH_USER/jx-kaniko

rm -rf jx-kaniko

rm -rf ~/.jx/environments/$GH_USER/environment-jx-rocks-*
```
